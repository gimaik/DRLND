\documentclass[12pt,twoside]{article}

\newcommand{\reporttitle}{Project 2: Continuous Control}
\newcommand{\reportauthor}{Thomas Teh}
\newcommand{\reporttype}{Project Report}
\newcommand{\cid}{0124 3008}

% include files that load packages and define macros
\input{includes} % various packages needed for maths etc.
\input{notation} % short-hand notation and macros
\DontPrintSemicolon

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% front page
\input{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%% Main document

\section{Deep Deterministic Policy Gradient (DDPG)}
I have implemented DDPG, as introduced by \cite{lillicrap2015continuous}, to solve this problem. The pseudo-code for DDPG is given in Algorithm 1. 


The action-value function describes the expected return after taking an action $a_t$ in state $s_t$ and thereafter following policy $\pi$:
\begin{align*}
	Q^\pi(s_t,a_t)	& = \mathbb{E}_{\pi}\left[R_t \vert s_t, a_t\right]\\
						& = \mathbb{E}_{\pi}\left[\sum_{i=t}^T \gamma^{i-t}r(s_i,a_i)\vert s_t, a_t\right]\\
\end{align*}

The action-value function follows the Bellman equation:
\begin{align*}
	Q^\pi(s_t,a_t) &=  \mathbb{E}_\pi\left[r(s_t,a_t) + \gamma \mathbb{E}_\pi\left[Q^\pi(s_{t+1}, a_{t+1})\right] \right]
\end{align*}

Given that the target policy is deterministic, we can described it as $\mu: \mathcal{S} \rightarrow \mathcal{A}$
\begin{align*}
	Q^\mu(s_t,a_t) &=  \mathbb{E}_\pi\left[r(s_t,a_t) + \gamma Q^\mu(s_{t+1}, \mu(s_{t+1})) \right]
\end{align*}



In DDPG, we use non-linear function approximator to learn both the $Q(s_t, a_t)$ action-value function and the policy $\mu(s_t)$. 


Firstly, the action-value function is learned using Q-learning, whereby $\mu(s_t) = \text{arg max}_{a\in \mathcal{A}} Q(s_t,a_t)$. Similar to the DQN algorithm introduced by \cite{mnih2015human}, the action-value function $Q(s_t, a_t)$ is learned by minimizing the following loss function:
\begin{align*}
	L(\theta^Q) = \mathbb{E}_{\pi^\prime}\left[(Q(s_t,a_t)-y_t)^2\right]
\end{align*}
where
\begin{align*}
	y_t = r(s_t,a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1})\vert \theta^Q)
\end{align*}

Secondly, the actor network is learned by using the DPG algorithm introduced by \cite{silver2014deterministic}. The actor is updated by the following:
\begin{align*}
	\nabla_{\theta^\mu} J & \approx \mathbb{E}_{\pi^\prime}\left[\nabla_{\theta^\mu}Q(s,a\vert \theta^Q)\vert_{s=s_t, a=\mu(s_t\vert\theta^\mu)}\right]\\
	& = \mathbb{E}_{\pi^\prime}\left[\nabla_a Q(s,a\vert \theta^Q)\vert_{s=s_t, a=\mu(s_t)}\nabla_{\theta^\mu} \mu(s\vert \theta^\mu)\vert_{s=s_t}\right]\\
\end{align*}

One final issue on DDPG is exploration. Exploration is introduced to the algorithm by including some noise in the action.
\begin{align*}
	a_t = \mu(s_t\vert \theta^\mu) + \mathcal{N_t}.
\end{align*}

$\mathcal{N}_t $ is an Ornstein-Uhlenbeck process
\begin{align*}
	d\mathcal{N}_t = \theta_{ou}(\mu_{ou} - \mathcal{N}_t)dt + \sigma_{ou} dW_t
\end{align*}
where
$\theta_{ou}$ is the mean-reversion rate, $\mu_{ou}$ is the long-term mean, $sigma_{ou}$ is the volatility and  $W_t$ is a Brownian motion.


\newpage

\begin{algorithm}[H]
\SetAlgoLined
\textbf{Initialization:}\\
Randomly initialize critic network $Q\left(s,a \vert \theta^Q\right)$ and actor $\mu\left(s\vert \theta^\mu\right)$ with weights $\theta^Q$ and $\theta^\mu$.\\
Initialize target network $Q^\prime$ and $\mu^\prime$ with weights $\theta^{Q^\prime} \leftarrow \theta^Q$ and $\theta^{\mu^\prime} \leftarrow \theta^\mu$.\\
Initialize replay buffer $R$.\\
\;
\For{episode = 1:M}{
	Initialize a random process $\mathcal{N}$ for action exploration.\\
	Receive initial observation state $s_1$.\\
	\;
	\For{t=1:T}{
		Selection action $a_t = \mu(s_t\vert \theta^\mu) + \mathcal{N}$ according to the current policy and exploration noise.\\
		Execute action $a_t$ and observe reward $r_t$ and observer new state $s_{t+1}$.\\
		Store transition $(s_t, a_t, r_t, s_{t+1})$ in $R$.\\
		Sample a random minibatch of $N$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$.\\
		Set $y_i = r_i + \gamma Q^\prime(s_{t+1},  \mu^\prime(s_{i+1}\vert \theta^{\mu^\prime})\vert \theta^{Q^\prime})$.\\
		Update the critic network by minimizing:
		\begin{align*}
			L = \frac{1}{N} \sum_i \left(y_i - Q(s_i, a_i\vert \theta^Q)\right)^2.
		\end{align*}
		Update the actor policy using the sampled policy gradient:
		\begin{align*}
			\nabla_{\theta^\mu} J \approx \frac{1}{N} \sum_i \nabla_a Q(s,a\vert\theta^Q) \vert_{s=s_i, a=\mu(s_i)}\nabla_{\theta^\mu} \mu(s\vert\theta^\mu)\vert_{s_i}.
		\end{align*}
		Update the target networks:
		\begin{align*}
			\theta^{Q^\prime} & \leftarrow \tau \theta^Q + (1-\tau)\theta^{Q^\prime} \\
			\theta^{\mu^\prime} & \leftarrow \tau \theta^\mu + (1-\tau)\theta^{\mu^\prime} \\
		\end{align*}
	}	
}

 \caption{Deep Deterministic Policy Gradient (DDPG)}
\end{algorithm}

\newpage

\section{Modification to DDPG}
In the project, I specifically tackled the problem with 20 agents. DDPG was originally designed for a single agent. However, since all 20 agents have the same tasks, the tuple $(s_t, a_t, r_t, s_{t+1})$ resulting from an action $a_t$ taken by the different agents are stored in the experience replay buffer. While this does not constitute a full distributed learning algorithm such as A3C or D4PG, it was sufficient to do so in order to solve the problem.

\section{Architectures of the Actor and Critic Network}

The actor network, as shown in Figure \ref{fig:actor_architecture} consists of the following:
	\begin{itemize}
		\item Input layer: The input to the network are the states of the environments observable by the agents.
		\item Hidden layer: The hidden layer is made up of 256 neuron units with exponential linear units.
		\item Output layer: The network outputs the action taken by the agent, which consists of 4 numbers, corresponding to the torque applicable on to two joints.
	\end{itemize}


\begin{figure}[H]
	\begin{center}
		\includegraphics[width = 0.625\hsize]{./figures/Actor.png} 
		\caption{Architecture of the actor network.} % caption of the figure
		\label{fig:actor_architecture} % a label. When we refer to this label from the text, the figure %number is included automatically
	\end{center}
\end{figure}

The critic network, as shown in Figure \ref{fig:critic_architecture} consists of the following:
	\begin{itemize}
		\item Input layer: The input to the network are the states of the environments observable by the agents.
		\item Hidden layer: Hidden layer 1 takes in the states observable by the agent as input. Hidden layer 2 takes both the output from hidden layer 1 and the action as inputs. Hidden layer 3 takes the output of hidden layer 2 as input.
		\item Output layer: The network outputs the value function of the state-action input.
	\end{itemize}
	
\begin{figure}[H]
	\begin{center}
		\includegraphics[width = 1\hsize]{./figures/Critic.png} 
		\caption{Architecture of the critic network.} % caption of the figure
		\label{fig:critic_architecture} % a label. When we refer to this label from the text, the figure %number is included automatically
	\end{center}
\end{figure}

While in \citep{lillicrap2015continuous}, the authors mentioned the application of batch normalization with ReLU activation functions for their actor-critic networks. I found that the training process is more stable by using exponential linear units (ELUs) introduced by \cite{clevert2015fast}. The ELUs retain most of the characteristics of ReLUs but they have an implicit regularization effect which made training much more stable.

\section{Hyperparameters}
The hyperparameters used in training the DDPG algorithm are listed below:
\begin{center}
	\begin{tabular}{|l|l|}
	\hline
		\textbf{Hyperparameter}							& \textbf{Value}\\\hline
		Actor learning rate, $\alpha^\mu$				& 0.0010 \\
		Critic learning rate, $\alpha^Q$					& 0.0010 \\
		Discount factor, $\gamma$						& 0.9950 \\
		Buffer size												& 3,000,000\\
		Minibatch size											& 128\\
		Soft update weights, $\tau$						& 0.0010\\
		Update frequency										& Every 20 time steps\\
		Number of updates									& 10 updates\\
		Long term mean for noise, $\mu_{ou}$		& 0.00\\
		Mean reversion rate for noise, $\theta_{ou}$	& 0.15\\
		Volatility for noise, $\sigma_{ou}$				& 0.20\\\hline
	\end{tabular}
\end{center}

\section{Results}
	
The average rewards during the training process is shown in Figure \ref{fig:results}. The DDPG algorithm achieves a mean score of 34.50 (exceeding the required 30), across 100 episodes and all 20 agents after 737 episodes. 
	
\begin{figure}[H]
	\begin{center}
		\includegraphics[width = 0.8\hsize]{./figures/results.png} 
		\caption{Average reward vs number of episodes} % caption of the figure
		\label{fig:rewards} % a label. When we refer to this label from the text, the figure %number is included automatically
	\end{center}
\end{figure}


\section{Ideas for Future Work}
The solution of the problem can potentially improved by applying different algorithms. The applicable of DDPG in this problem is not parallelized across the 20 agents. Essentially, each agent would contribute to the memory for experience replay and then the parameter optimization update is done based on those replays. For future work, I would explore implementations that can be parallelized such as A3C and D4PG in order to speed up the training process. Also, for comparison purposes, I would implement PPO as well.


\bibliography{reference}
\bibliographystyle{apalike}


\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
